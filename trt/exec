#!/usr/bin/env python

import tensorrt as trt

import argparse
import os
import signal
import sys
import time

# <https://github.com/NVIDIA/TensorRT/blob/main/samples/python/common.py>
from common_runtime import allocate_buffers, do_inference
from logger import Logger

parser = argparse.ArgumentParser()
parser.add_argument('engine')
parser.add_argument('-n', '--batch-size', default=1, type=int)
args = parser.parse_args()

logger = Logger()
with open(args.engine, 'rb') as f, trt.Runtime(logger) as rt:
    engine = rt.deserialize_cuda_engine(f.read())
inputs, outputs, bindings, stream = allocate_buffers(engine, 0)
context = engine.create_execution_context()
for i in range(engine.num_io_tensors):
    binding = engine[i]
    if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:
        shape = engine.get_tensor_shape(binding)
        assert all(x >= 0 for x in shape[1:])
        if shape[0] < 0:
            context.set_input_shape(
                binding,
                [args.batch_size] + list(shape[1:]),
            )

done = False
def handler(*_):
    global done
    done = True
for signum in signal.SIGTERM, signal.SIGINT: signal.signal(signum, handler)

while not done:
    t_i = time.time()
    do_inference(context, engine, bindings, inputs, outputs, stream)
    t_f = time.time()
    try: print(t_f, t_f - t_i, flush=True)
    except BrokenPipeError:
        devnull = os.open(os.devnull, os.O_WRONLY)
        os.dup2(devnull, sys.stdout.fileno())
        break
